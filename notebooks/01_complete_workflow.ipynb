{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete AuDHD Correlation Study Workflow\n",
    "\n",
    "This notebook demonstrates the complete pipeline from data loading to results visualization.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Load multi-omics data\n",
    "2. Preprocess and harmonize\n",
    "3. Integrate modalities\n",
    "4. Cluster samples\n",
    "5. Validate clusters\n",
    "6. Biological interpretation\n",
    "7. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from audhd_correlation.data import load_multiomics, align_multiomics\n",
    "from audhd_correlation.preprocess import preprocess_pipeline\n",
    "from audhd_correlation.integrate import integrate_multiomics\n",
    "from audhd_correlation.modeling import perform_clustering, create_embedding\n",
    "from audhd_correlation.validation import validate_clusters\n",
    "from audhd_correlation.biological import compute_cluster_signatures\n",
    "from audhd_correlation.viz import plot_embedding, plot_heatmap\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load genomic, clinical, and metabolomic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_multiomics(\n",
    "    genomic_path=\"data/genomic/genotypes.vcf\",\n",
    "    clinical_path=\"data/clinical/phenotypes.csv\",\n",
    "    metabolomic_path=\"data/metabolomic/metabolites.csv\"\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(data)} modalities:\")\n",
    "for modality, df in data.items():\n",
    "    print(f\"  {modality}: {df.shape[0]} samples × {df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align samples across modalities\n",
    "aligned_data = align_multiomics(data)\n",
    "\n",
    "n_samples = len(aligned_data['genomic'])\n",
    "print(f\"After alignment: {n_samples} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "Impute missing values, normalize, and apply quality control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess each modality\n",
    "preprocessed = preprocess_pipeline(\n",
    "    aligned_data,\n",
    "    impute_method='knn',\n",
    "    scale_method='standard',\n",
    "    batch_correct=True,\n",
    "    batch_column='site'\n",
    ")\n",
    "\n",
    "print(\"Preprocessing complete:\")\n",
    "for modality, df in preprocessed.items():\n",
    "    missing_pct = (df.isna().sum().sum() / df.size) * 100\n",
    "    print(f\"  {modality}: {missing_pct:.2f}% missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Integration\n",
    "\n",
    "Integrate modalities using MOFA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate using MOFA\n",
    "integrated = integrate_multiomics(\n",
    "    preprocessed,\n",
    "    method='mofa',\n",
    "    n_factors=15\n",
    ")\n",
    "\n",
    "print(f\"Integrated data shape: {integrated['factors'].shape}\")\n",
    "print(f\"Variance explained: {integrated['variance_explained']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variance explained per modality\n",
    "var_df = pd.DataFrame(integrated['variance_per_modality'])\n",
    "var_df.plot(kind='bar', figsize=(10, 5))\n",
    "plt.title('Variance Explained per Modality')\n",
    "plt.ylabel('R² Score')\n",
    "plt.xlabel('Factor')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering\n",
    "\n",
    "Identify patient subtypes using HDBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform clustering\n",
    "cluster_result = perform_clustering(\n",
    "    integrated['factors'],\n",
    "    method='hdbscan',\n",
    "    min_cluster_size=20\n",
    ")\n",
    "\n",
    "labels = cluster_result['labels']\n",
    "n_clusters = cluster_result['n_clusters']\n",
    "\n",
    "print(f\"Found {n_clusters} clusters\")\n",
    "print(f\"Cluster sizes: {np.bincount(labels[labels >= 0])}\")\n",
    "print(f\"Noise points: {(labels == -1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UMAP embedding\n",
    "embedding = create_embedding(\n",
    "    integrated['factors'],\n",
    "    method='umap',\n",
    "    n_neighbors=15\n",
    ")\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c=labels,\n",
    "    cmap='tab10',\n",
    "    s=50,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.title(f'Patient Clusters (n={n_clusters})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation\n",
    "\n",
    "Validate cluster quality and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute validation metrics\n",
    "validation = validate_clusters(\n",
    "    integrated['factors'],\n",
    "    labels,\n",
    "    n_bootstrap=100\n",
    ")\n",
    "\n",
    "print(\"=== Validation Results ===\")\n",
    "print(f\"Silhouette Score: {validation['silhouette']:.3f}\")\n",
    "print(f\"Calinski-Harabasz: {validation['calinski_harabasz']:.1f}\")\n",
    "print(f\"Davies-Bouldin: {validation['davies_bouldin']:.3f}\")\n",
    "print(f\"Bootstrap Stability (ARI): {validation['stability_ari']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clinical Characterization\n",
    "\n",
    "Compare clusters by clinical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clinical data\n",
    "clinical = aligned_data['clinical']\n",
    "\n",
    "# Add cluster labels\n",
    "clinical['cluster'] = labels\n",
    "\n",
    "# Remove noise points for analysis\n",
    "clinical_clustered = clinical[clinical['cluster'] >= 0]\n",
    "\n",
    "# Compare age across clusters\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Age\n",
    "clinical_clustered.boxplot(column='age', by='cluster', ax=axes[0])\n",
    "axes[0].set_title('Age by Cluster')\n",
    "axes[0].set_xlabel('Cluster')\n",
    "axes[0].set_ylabel('Age (years)')\n",
    "\n",
    "# Severity score\n",
    "clinical_clustered.boxplot(column='severity_score', by='cluster', ax=axes[1])\n",
    "axes[1].set_title('Severity Score by Cluster')\n",
    "axes[1].set_xlabel('Cluster')\n",
    "\n",
    "# Diagnosis distribution\n",
    "diag_counts = clinical_clustered.groupby(['cluster', 'diagnosis']).size().unstack(fill_value=0)\n",
    "diag_counts.plot(kind='bar', ax=axes[2], stacked=True)\n",
    "axes[2].set_title('Diagnosis Distribution by Cluster')\n",
    "axes[2].set_xlabel('Cluster')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].legend(title='Diagnosis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Biological Interpretation\n",
    "\n",
    "Identify cluster signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cluster signatures\n",
    "signatures = compute_cluster_signatures(\n",
    "    preprocessed,\n",
    "    labels,\n",
    "    method='limma'\n",
    ")\n",
    "\n",
    "print(\"Cluster signatures:\")\n",
    "for cluster_id, sig in signatures.items():\n",
    "    if cluster_id >= 0:  # Skip noise\n",
    "        print(f\"  Cluster {cluster_id}: {len(sig)} significant features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results\n",
    "\n",
    "Export results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cluster assignments\n",
    "results_df = pd.DataFrame({\n",
    "    'sample_id': integrated['factors'].index,\n",
    "    'cluster': labels,\n",
    "    'umap1': embedding[:, 0],\n",
    "    'umap2': embedding[:, 1]\n",
    "})\n",
    "\n",
    "results_df.to_csv('outputs/cluster_assignments.csv', index=False)\n",
    "\n",
    "# Save validation metrics\n",
    "import json\n",
    "with open('outputs/validation_metrics.json', 'w') as f:\n",
    "    json.dump(validation, f, indent=2)\n",
    "\n",
    "print(\"Results saved to outputs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This workflow demonstrates:\n",
    "\n",
    "1. ✓ Loading and harmonizing multi-omics data\n",
    "2. ✓ Preprocessing with imputation and normalization\n",
    "3. ✓ Integration using MOFA\n",
    "4. ✓ Clustering with HDBSCAN\n",
    "5. ✓ Validation of cluster quality\n",
    "6. ✓ Clinical characterization\n",
    "7. ✓ Biological interpretation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Perform pathway enrichment analysis\n",
    "- Create comprehensive report\n",
    "- Validate on independent cohort"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}